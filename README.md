# SparseTrack
####  SparseTrack integrated with the [YOLOv8](https://github.com/ultralytics/ultralytics) detector. 

**Here includes the implementation of the [Detectron2](https://github.com/facebookresearch/detectron2) version of YOLOv8 detector and its usage in SparseTrack for tracking.**

## Tracking performance
### Results on MOT challenge test set with yolov8_l detector
| Dataset    | HOTA | MOTA | IDF1 | MT | ML | FP | FN | IDs | FPS |
|------------|-------|-------|------|------|-------|-------|------|------|-----|
|MOT17       | 65.7 | 80.0 | 79.9 | 53.0% | 14.5% | 24327 | 87297 | 1095 | 32  |


## Copmlie GMC

You can refer to [complie GMC](https://github.com/hustvl/SparseTrack#compile-gmcgloble-motion-compensation-module).

## Usage
```shell
git clone -b v8 https://github.com/hustvl/SparseTrack.git
# rename 'SparseTrack' to 'yolov8'
cd yolov8
pip install -r requirements.txt
pip install Cython  
pip install cython_bbox
```

## Data preparation
Download [MOT17](https://motchallenge.net/), [MOT20](https://motchallenge.net/), [CrowdHuman](https://www.crowdhuman.org/), [Cityperson](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md), [ETHZ](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md) and put them under ROOT/ in the following structure:
```
ROOT
   |
   |â€”â€”â€”â€”â€”â€”yolov8(repo)
   |           â””â€”â€”â€”â€”â€”mix
   |                  â””â€”â€”â€”â€”â€”â€”mix_17/annotations
   |                  â””â€”â€”â€”â€”â€”â€”mix_20/annotations
   |                  â””â€”â€”â€”â€”â€”â€”ablation_17/annotations
   |                  â””â€”â€”â€”â€”â€”â€”ablation_20/annotations
   |â€”â€”â€”â€”â€”â€”MOT17
   |        â””â€”â€”â€”â€”â€”â€”train
   |        â””â€”â€”â€”â€”â€”â€”test
   â””â€”â€”â€”â€”â€”â€”crowdhuman
   |         â””â€”â€”â€”â€”â€”â€”Crowdhuman_train
   |         â””â€”â€”â€”â€”â€”â€”Crowdhuman_val
   |         â””â€”â€”â€”â€”â€”â€”annotation_train.odgt
   |         â””â€”â€”â€”â€”â€”â€”annotation_val.odgt
   â””â€”â€”â€”â€”â€”â€”MOT20
   |        â””â€”â€”â€”â€”â€”â€”train
   |        â””â€”â€”â€”â€”â€”â€”test
   â””â€”â€”â€”â€”â€”â€”Citypersons
   |        â””â€”â€”â€”â€”â€”â€”images
   |        â””â€”â€”â€”â€”â€”â€”labels_with_ids
   â””â€”â€”â€”â€”â€”â€”ETHZ
   |        â””â€”â€”â€”â€”â€”â€”eth01
   |        â””â€”â€”â€”â€”â€”â€”...
   |        â””â€”â€”â€”â€”â€”â€”eth07
   â””â€”â€”â€”â€”â€”â€”dancetrack
               â””â€”â€”â€”â€”â€”â€”train
               â””â€”â€”â€”â€”â€”â€”train_seqmap.txt
               â””â€”â€”â€”â€”â€”â€”test
               â””â€”â€”â€”â€”â€”â€”test_seqmap.txt
               â””â€”â€”â€”â€”â€”â€”val
               â””â€”â€”â€”â€”â€”â€”val_seqmap.txt

   
```
Then, you need to turn the datasets to COCO format and mix different training data:
```
cd <ROOT>/yolov8
python3 tools/convert_mot17_to_coco.py
python3 tools/convert_mot20_to_coco.py
python3 tools/convert_crowdhuman_to_coco.py
python3 tools/convert_cityperson_to_coco.py
python3 tools/convert_ethz_to_coco.py
python3 tools/convert_dance_to_coco.py
```
Creating different training mix_data:
```
cd <ROOT>/yolov8

# training on CrowdHuman and MOT17 half train, evaluate on MOT17 half val.
python3 tools/mix_data_ablation.py

# training on CrowdHuman and MOT20 half train, evaluate on MOT20 half val.
python3 tools/mix_data_ablation_20.py

# training on MOT17, CrowdHuman, ETHZ, Citypersons, evaluate on MOT17 train.
python3 tools/mix_data_test_mot17.py

# training on MOT20 and CrowdHuman, evaluate on MOT20 train.
python3 tools/mix_data_test_mot20.py
``` 


## Training
All training is conducted on a unified script. You need to change the **VAL_JSON** and **VAL_PATH** in [register_data.py](https://github.com/hustvl/SparseTrack/blob/main/register_data.py), and then run as followsï¼š
```
# training on MOT17, CrowdHuman, ETHZ, Citypersons, evaluate on MOT17 train set.
CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --num-gpus 4  --config-file mot17_train_config.py 


# training on MOT20, CrowdHuman, evaluate on MOT20 train set.
CUDA_VISIBLE_DEVICES=0,1,2,3 python train.py --num-gpus 4  --config-file mot20_train_config.py 
```
**Notes**: 
For MOT20, you need to clip the bounding boxes inside the image.

Add clip operation in line 138-139 in [data_augment.py](https://github.com/hustvl/SparseTrack/blob/v8/datasets/data/data_augment.py), line 118-121 in [mosaicdetection.py](https://github.com/hustvl/SparseTrack/blob/v8/datasets/data/datasets/mosaicdetection.py), line 213-221 in mosaicdetection.py, line 70-73 in [boxes.py](https://github.com/hustvl/SparseTrack/blob/v8/utils/boxes.py).

## Tracking
All tracking experimental scripts are run in the following manner. You first place the model weights in the **<ROOT/yolov8/pretrain/>**, and change the **VAL_JSON** and **VAL_PATH** in [register_data.py](https://github.com/hustvl/SparseTrack/blob/v8/register_data.py).
```
# tracking on mot17 train set or test set
CUDA_VISIBLE_DEVICES=0 python3 track.py  --num-gpus 1  --config-file mot17_track_cfg.py 
```

## Citation -->
If you find SparseTrack is useful in your research or applications, please consider giving us a star ðŸŒŸ and citing it by the following BibTeX entry.
```bibtex
@inproceedings{SparseTrack,
  title={SparseTrack: Multi-Object Tracking by Performing Scene Decomposition based on Pseudo-Depth},
  author={Liu, Zelin and Wang, Xinggang and Wang, Cheng and Liu, Wenyu and Bai, Xiang},
  journal={arXiv preprint arXiv:2306.05238},
  year={2023}
}
```

## Acknowledgements
A large part of the code is borrowed from [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX), [FairMOT](https://github.com/ifzhang/FairMOT), [ByteTrack](https://github.com/ifzhang/ByteTrack), [BoT-SORT](https://github.com/NirAharon/BOT-SORT), [Detectron2](https://github.com/facebookresearch/detectron2), [YOLOV8](https://github.com/ultralytics/ultralytics).
 Many thanks for their wonderful works.

